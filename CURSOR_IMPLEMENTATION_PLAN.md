# CURSOR IMPLEMENTATION PLAN
## Context-Aware Skills for SWE-bench Optimization

**Goal:** Implement long-running agent support + skills framework to improve MAKER's SWE-bench resolve rate from ~30% to >50%

**Approach:** Build skills that teach MAKER how to solve coding problems better, NOT user-facing features

---

## PHASE 1: Foundation - Long-Running Support
**Duration:** 3-5 days
**Files:** 4 new, 2 modified

### Task 1.1: Create ProgressTracker
**File:** `orchestrator/progress_tracker.py`

**Purpose:** Track what MAKER has accomplished across sessions

**Key Methods:**
```python
class ProgressTracker:
    def __init__(self, workspace_path: Path)
    def log_progress(self, message: str)  # Append to claude-progress.txt
    def load_feature_list(self) -> List[Dict]  # Read feature_list.json
    def update_feature_status(self, name: str, passes: bool)
    def get_next_feature(self) -> Optional[Dict]  # Highest priority incomplete
```

**Implementation Notes:**

- Use `claude-progress.txt` for append-only logs
- Use `feature_list.json` structure: `{"features": [{"name": "...", "description": "...", "passes": false, "priority": 1}]}`
- Thread-safe file writes (use file locking)
- Auto-create files if they don't exist

**Test:** Create `tests/test_progress_tracker.py` with tests for log writing, feature updates, and priority sorting

---

### Task 1.2: Add Session Resumability

**File:** `orchestrator/session_manager.py`

**Purpose:** Resume long-running sessions with full context

**Key Methods:**

```python
class SessionManager:
    def resume_session(self, session_id: str) -> Dict[str, str]
    def create_resume_context(self) -> str  # Returns orientation prompt
    def verify_clean_state(self) -> bool  # Check if safe to continue
```

**Resume Context Template:**

```
You are resuming work on this project.

Working directory: {cwd}

Recent progress (last 10 entries):
{progress_log}

Recent git commits (last 5):
{git_log}

Next feature to implement:
{next_feature}

Continue working on this feature. Do not start new features.
```

**Test:** Verify resume picks up correct feature and includes git context

---

### Task 1.3: Implement Checkpointing

**File:** `orchestrator/checkpoint_manager.py`

**Purpose:** Create clean checkpoints with git commits

**Key Methods:**

```python
class CheckpointManager:
    def create_checkpoint(self, feature_name: str, code: str) -> Dict
    def verify_tests_pass(self) -> bool
    def commit_changes(self, message: str) -> str  # Returns commit hash
```

**Checkpoint Flow:**

1. Verify all tests pass
2. Git commit with descriptive message
3. Update feature status to passes=true
4. Log progress
5. Save session state to Redis

**Commit Message Format:**

```
feat: Complete {feature_name}

{auto-generated summary from code changes}

ðŸ¤– Generated by MAKER Multi-Agent System
```

**Test:** Verify checkpoint creates git commit and updates feature status

---

### Task 1.4: Integrate with Orchestrator

**File:** `orchestrator/orchestrator.py` (MODIFY)

**Changes:**

```python
class Orchestrator:
    def __init__(self, ...):
        # Add new components
        self.progress_tracker = ProgressTracker(workspace_path)
        self.session_manager = SessionManager(self.progress_tracker)
        self.checkpoint_manager = CheckpointManager(self.progress_tracker)

    async def resume_session(self, session_id: str) -> AsyncGenerator:
        """Resume long-running session"""
        context = self.session_manager.create_resume_context()
        return await self.orchestrate_workflow(session_id, context)

    async def checkpoint_session(self, session_id: str, feature_name: str):
        """Create clean checkpoint"""
        state = TaskState.from_redis(self.redis, session_id)
        return await self.checkpoint_manager.create_checkpoint(
            feature_name, state.code
        )
```

**API Endpoints to Add (api_server.py):**

```python
@app.post("/api/session/{session_id}/resume")
async def resume_session_endpoint(session_id: str):
    return StreamingResponse(
        orchestrator.resume_session(session_id),
        media_type="text/event-stream"
    )

@app.post("/api/session/{session_id}/checkpoint")
async def checkpoint_endpoint(session_id: str, feature_name: str):
    return await orchestrator.checkpoint_session(session_id, feature_name)
```

**Environment Variables:**

```bash
ENABLE_LONG_RUNNING=true
WORKSPACE_DIR=/app/workspace
```

**Test:** Full integration test with resume â†’ work â†’ checkpoint flow

---

## PHASE 2: Skills Framework for SWE-bench

**Duration:** 5-7 days
**Files:** 6 new, 3 modified

### Task 2.1: Create Core Skill Structure

**Directory:** `skills/`

**Create example skill:** `skills/regex-pattern-fixing/SKILL.md`

```markdown
---
name: regex-pattern-fixing
description: Fix regex patterns using proven patterns from SWE-bench tasks
category: core-coding
applies_to: ["regex", "pattern", "validation", "re.compile"]
swe_bench_examples: ["marshmallow-1359", "flask-2234"]
success_rate: 0.65
usage_count: 0
created: 2024-12-01
---

# Regex Pattern Fixing

## Recognition
This skill applies when:
- Issue mentions "regex", "pattern", "validation"
- Code contains `import re` or `re.compile()`
- Error is `re.error` or pattern mismatch

## Proven Patterns

### Email Validation
```python
# âŒ WRONG (from failed SWE-bench attempts)
r'[\w.]+@[\w.]+'

# âœ… RIGHT (from successful tasks)
r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
```

### Common Edge Cases
- Empty string: Always test `re.match(pattern, "")`
- Special chars: Test `@`, `.`, `-`, `_`
- Case sensitivity: Use `re.IGNORECASE` or `[a-zA-Z]`

## Anti-Patterns
- Using `.` without escaping for literal dot
- Forgetting `^` and `$` anchors
- Using greedy `.*` instead of lazy `.*?`

## Verification Checklist
- [ ] Test empty string
- [ ] Test special characters
- [ ] All FAIL_TO_PASS tests pass
- [ ] All PASS_TO_PASS tests still pass
```

**Create 4 more core skills:**

- `skills/test-driven-bug-fixing/SKILL.md`
- `skills/python-ast-refactoring/SKILL.md`
- `skills/error-message-reading/SKILL.md`
- `skills/django-migration-patterns/SKILL.md`

---

### Task 2.2: Implement SkillLoader

**File:** `orchestrator/skill_loader.py`

**Purpose:** Load and parse skills from SKILL.md files

**Key Methods:**

```python
class SkillLoader:
    def __init__(self, skills_dir: Path):
        self.skills_dir = skills_dir
        self.skills_cache: Dict[str, Skill] = {}

    def load_skill(self, skill_name: str) -> Skill
    def load_all_skills(self) -> List[Skill]
    def parse_skill_file(self, path: Path) -> Skill
```

**Skill Dataclass:**

```python
@dataclass
class Skill:
    name: str
    description: str
    category: str
    applies_to: List[str]
    instructions: str
    metadata: Dict[str, Any]
```

**Implementation:**

- Use YAML parser for frontmatter
- Cache loaded skills in memory
- Validate required fields (name, description)

**Test:** Load all skills and verify parsing

---

### Task 2.3: Implement SkillMatcher

**File:** `orchestrator/skill_matcher.py`

**Purpose:** Find relevant skills for a given task using RAG

**Key Methods:**

```python
class SkillMatcher:
    def __init__(self, skill_loader: SkillLoader, rag_service):
        self.skill_loader = skill_loader
        self.rag = rag_service

    def find_relevant_skills(self, task_description: str, top_k=3) -> List[Skill]
    def index_all_skills(self)  # Index skills in RAG
    def calculate_relevance(self, task: str, skill: Skill) -> float
```

**Matching Strategy:**

1. **Keyword matching** - Check `applies_to` list
2. **Semantic search** - RAG query on skill descriptions
3. **Category filtering** - Prefer core-coding skills
4. **Success rate weighting** - Boost high-performing skills

**Relevance Score:**

```python
score = (
    keyword_match * 0.3 +
    semantic_similarity * 0.4 +
    success_rate * 0.2 +
    usage_count_boost * 0.1
)
```

**Test:** Verify regex task matches regex-pattern-fixing skill

---

### Task 2.4: Integrate Skills into Agents

**File:** `orchestrator/orchestrator.py` (MODIFY)

**Changes to `call_agent()` method:**

```python
async def call_agent(self, agent: AgentName, system_prompt: str, user_prompt: str, ...):
    # Find relevant skills
    if self.enable_skills:
        skills = self.skill_matcher.find_relevant_skills(user_prompt, top_k=2)

        if skills:
            # Augment system prompt with skills
            skills_section = "\n\n## Available Proven Patterns\n"
            for skill in skills:
                skills_section += f"\n### {skill.name}\n"
                skills_section += f"{skill.instructions}\n"

            system_prompt = system_prompt + skills_section

            # Log skill usage
            for skill in skills:
                self._log_skill_usage(skill.name)

    # Continue with normal agent call
    ...
```

**Skill Usage Tracking:**

```python
def _log_skill_usage(self, skill_name: str):
    key = f"skills:usage:{skill_name}"
    self.redis.incr(key)
    self.redis.expire(key, 86400 * 30)  # 30 day TTL
```

**Test:** Verify skills are injected into Coder agent prompt

---

### Task 2.5: Test on SWE-bench Subset

**Script:** `scripts/test_skills_impact.sh`

**Purpose:** Measure skill impact on 20-task subset

**Test Groups:**

- 10 tasks WITHOUT skills (baseline)
- 10 tasks WITH skills (enhanced)

**Metrics to Track:**

- Resolve rate improvement
- Skill usage frequency
- Which skills were most helpful

**Expected Result:** +8-12% resolve rate improvement

---

## PHASE 3: Incremental Skill Learning

**Duration:** 5-7 days
**Files:** 3 new, 2 modified

### Task 3.1: Implement SkillExtractor

**File:** `orchestrator/skill_extractor.py`

**Purpose:** Extract reusable patterns from successful tasks

**Key Methods:**

```python
class SkillExtractor:
    async def extract_skill_from_task(self, task_id: str) -> Optional[Skill]
    def is_skill_worthy(self, state: TaskState) -> bool
    def generate_skill_definition(self, state: TaskState) -> str
    def detect_pattern_type(self, code: str) -> str
```

**Extraction Criteria:**

```python
def is_skill_worthy(self, state: TaskState) -> bool:
    return (
        state.review_feedback.get('status') == 'approved' and
        len(state.code) > 200 and  # Non-trivial
        self._has_reusable_pattern(state) and
        not self._is_one_off_solution(state)
    )
```

**Pattern Detection:**

- Regex usage â†’ regex-pattern-fixing
- AST manipulation â†’ python-ast-refactoring
- Django models â†’ django-migration-patterns
- Test-driven â†’ test-driven-bug-fixing

**Skill Generation Process:**

1. Analyze successful code
2. Extract key patterns
3. Identify anti-patterns (what NOT to do)
4. Generate SKILL.md using Planner agent
5. Save to skills/ directory
6. Index in RAG

**Test:** Extract skill from sample successful task

---

### Task 3.2: Implement Skill Registry

**File:** `orchestrator/skill_registry.py`

**Purpose:** Manage skill lifecycle and statistics

**Registry Schema (Redis):**

```json
{
  "skills:registry": {
    "regex-pattern-fixing": {
      "name": "regex-pattern-fixing",
      "created": "2024-12-01T10:00:00",
      "usage_count": 23,
      "success_count": 15,
      "success_rate": 0.65,
      "last_used": "2024-12-01T15:30:00",
      "version": 2
    }
  }
}
```

**Key Methods:**

```python
class SkillRegistry:
    def register_skill(self, skill: Skill)
    def update_skill_stats(self, skill_name: str, success: bool)
    def get_skill_stats(self, skill_name: str) -> Dict
    def merge_similar_skills(self, skill1: str, skill2: str)
```

**Skill Evolution:**

- Track success rate per skill
- Merge similar skills (>90% semantic overlap)
- Version skills when updated
- Deprecate low-performing skills (<30% success rate)

**Test:** Verify skill stats update correctly

---

### Task 3.3: Auto-Apply Skills

**File:** `orchestrator/orchestrator.py` (MODIFY)

**Changes to `orchestrate_workflow()`:**

```python
async def orchestrate_workflow(self, task_id: str, user_input: str):
    # Check for similar previous tasks
    similar_skills = self.skill_matcher.find_relevant_skills(user_input)

    if similar_skills and similar_skills[0].relevance > 0.85:
        yield f"[SKILL] Found highly relevant skill: {similar_skills[0].name}\n"
        yield f"[SKILL] This pattern solved {similar_skills[0].usage_count} similar tasks before\n"
        yield f"[SKILL] Success rate: {similar_skills[0].success_rate:.0%}\n"

    # Continue with workflow, skills auto-injected in call_agent()
    ...

    # After completion, extract new skill if appropriate
    if enable_skill_learning and state.review_feedback.get('status') == 'approved':
        new_skill = await self.skill_extractor.extract_skill_from_task(task_id)
        if new_skill:
            yield f"[LEARNING] Extracted new skill: {new_skill.name}\n"
            self.skill_registry.register_skill(new_skill)
```

**Test:** Verify second similar task auto-applies learned skill

---

### Task 3.4: Skill Evolution Loop

**Script:** `scripts/evolve_skills.py`

**Purpose:** Refine skills based on usage data

**Process:**

1. Load all skill usage stats
2. Identify low-performing skills
3. Analyze what went wrong
4. Update skill instructions
5. Re-index in RAG

**Example Evolution:**

```markdown
# Version 1 (60% success rate)
## Regex Pattern
Use `.*` for matching

# Version 2 (75% success rate)
## Regex Pattern
Use `.*?` (lazy) instead of `.*` (greedy)
Added after analyzing 5 failures where greedy matching caused issues
```

**Test:** Verify skill version increments after evolution

---

## TESTING STRATEGY

### Unit Tests (Per Phase)

**Phase 1 Tests:**

- `tests/test_progress_tracker.py`
- `tests/test_session_manager.py`
- `tests/test_checkpoint_manager.py`

**Phase 2 Tests:**

- `tests/test_skill_loader.py`
- `tests/test_skill_matcher.py`
- `tests/test_skill_integration.py`

**Phase 3 Tests:**

- `tests/test_skill_extractor.py`
- `tests/test_skill_registry.py`
- `tests/test_skill_evolution.py`

### Integration Tests

**Test Scenario 1: Multi-Session Workflow**

```python
# Session 1: Start feature
session_id = "test_session_1"
result1 = await orchestrator.orchestrate_workflow(session_id, "Add authentication")
# Checkpoint
await orchestrator.checkpoint_session(session_id, "auth")

# Session 2: Resume and continue
result2 = await orchestrator.resume_session(session_id)
assert "Resuming work" in result2
```

**Test Scenario 2: Skill Application**

```python
# Task 1: Regex bug (creates skill)
result1 = await orchestrator.orchestrate_workflow("task1", "Fix email regex in validator")
assert "regex-pattern-fixing" in extracted_skills

# Task 2: Similar regex bug (uses skill)
result2 = await orchestrator.orchestrate_workflow("task2", "Fix URL regex in parser")
assert "Found highly relevant skill: regex-pattern-fixing" in result2
```

### SWE-bench Validation

**Baseline Run (No Skills):**

```bash
EE_MODE=false ENABLE_SKILLS=false \
bash tests/run_swe_bench_eval.sh 50 results/baseline
```

**Enhanced Run (With Skills):**

```bash
EE_MODE=true ENABLE_SKILLS=true \
bash tests/run_swe_bench_eval.sh 50 results/enhanced
```

**Compare Results:**

```bash
python scripts/compare_swe_bench_results.py \
  results/baseline/metrics.json \
  results/enhanced/metrics.json
```

**Expected Improvement:**

- Baseline: ~30% resolve rate
- Enhanced: ~40% resolve rate (+10%)

---

## ENVIRONMENT VARIABLES

Add to `docker-compose.yml`:

```yaml
environment:
  # Existing
  - EE_MODE=true
  - MAKER_NUM_CANDIDATES=5

  # New (Phase 1)
  - ENABLE_LONG_RUNNING=true
  - WORKSPACE_DIR=/app/workspace

  # New (Phase 2)
  - ENABLE_SKILLS=true
  - SKILLS_DIR=/app/skills

  # New (Phase 3)
  - ENABLE_SKILL_LEARNING=true
  - SKILL_EXTRACTION_THRESHOLD=0.7

volumes:
  # New: Persist progress files and skills
  - ./workspace:/app/workspace
  - ./skills:/app/skills
```

---

## SUCCESS METRICS

### Phase 1 Success Criteria

- [ ] Can resume session after interruption
- [ ] Progress logged to claude-progress.txt
- [ ] Features tracked in feature_list.json
- [ ] Git commits created at checkpoints
- [ ] Tests pass for all components

### Phase 2 Success Criteria

- [ ] 5 core skills created and loaded
- [ ] Skills injected into agent prompts
- [ ] Skill matching finds relevant skills
- [ ] +8-12% improvement on 50-task subset

### Phase 3 Success Criteria

- [ ] Skills auto-extracted from successful tasks
- [ ] Skill registry tracks usage stats
- [ ] Second similar task uses learned skill
- [ ] +15-20% improvement after 10 learning iterations

### Overall Target

**SWE-bench Lite Resolve Rate:**

- Baseline (current): ~30%
- After Phase 1: ~32% (+session resumability helps complex tasks)
- After Phase 2: ~42% (+skills provide proven patterns)
- After Phase 3: ~50% (+learning from experience)

**Goal: Beat GPT-4o Mini (31.7%), approach Claude 3.5 Sonnet (49.3%)**

---

## IMPLEMENTATION ORDER

### Week 1: Foundation

- Day 1-2: Task 1.1, 1.2 (ProgressTracker, SessionManager)
- Day 3-4: Task 1.3, 1.4 (Checkpointing, Integration)
- Day 5: Testing and documentation

### Week 2: Core Skills

- Day 1-2: Task 2.1, 2.2 (Create 5 skills, SkillLoader)
- Day 3-4: Task 2.3, 2.4 (SkillMatcher, Integration)
- Day 5: Task 2.5 (Test on SWE-bench subset)

### Week 3: Learning

- Day 1-2: Task 3.1 (SkillExtractor)
- Day 3-4: Task 3.2, 3.3 (Registry, Auto-apply)
- Day 5: Task 3.4 (Evolution, final testing)

---

## CURSOR WORKFLOW

For each task:

1. **Read this plan** for the specific task
2. **Create the file** with proper structure
3. **Implement methods** following patterns in orchestrator.py
4. **Add type hints** everywhere
5. **Write tests** in tests/ directory
6. **Run tests** with `pytest tests/test_<feature>.py`
7. **Commit** with conventional commit message
8. **Move to next task**

**Example Cursor prompts:**

```
"Implement Task 1.1: Create ProgressTracker class in orchestrator/progress_tracker.py
following the spec in CURSOR_IMPLEMENTATION_PLAN.md"

"Add tests for ProgressTracker in tests/test_progress_tracker.py"

"Integrate ProgressTracker into Orchestrator class per Task 1.4"
```

---

## REFERENCES

- Anthropic long-running agents: https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents
- Anthropic skills framework: https://github.com/anthropics/skills
- Gap analysis: `docs/context-engineering-skills-analysis.md`
- Skills clarification: `docs/skills-framework-clarification.md`
- SWE-bench integration: `docs/swe-bench-integration.md`

---

**READY FOR CURSOR IMPLEMENTATION**

## Before Starting

**Read:** `CURSOR_QUESTIONS_ANSWERED.md` for implementation clarifications

**Create directories:**

```bash
mkdir -p workspace skills
```

**Start with:** Task 1.1: ProgressTracker (no blockers, implement immediately)
