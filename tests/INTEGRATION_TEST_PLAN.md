# Integration Test Plan - Phases 1 & 2

## Purpose
Validate that Phase 1 (Long-Running Support) and Phase 2 (Skills Framework) work end-to-end with actual llama.cpp models running, not just mocked unit tests.

## Prerequisites

### 1. Models Running
```bash
# Start all llama.cpp servers
bash scripts/start-llama-servers.sh

# Verify all 6 models are running
for port in 8000 8001 8002 8003 8004 8005; do
  echo "Port $port:"
  curl -s http://localhost:$port/health | jq .
done

# Expected: All should return {"status": "ok"}
```

### 2. Supporting Services
```bash
# Start Redis and MCP server
docker compose up -d redis mcp-server

# Verify Redis
docker compose exec redis redis-cli ping
# Expected: PONG

# Verify MCP server
curl -s http://localhost:9001/health
# Expected: {"status": "healthy"}
```

### 3. Environment Setup
```bash
# Create required directories
mkdir -p workspace skills

# Set environment variables
export ENABLE_LONG_RUNNING=true
export WORKSPACE_DIR=/Users/anthonylui/BreakingWind/workspace
export ENABLE_SKILLS=true
export SKILLS_DIR=/Users/anthonylui/BreakingWind/skills
export EE_MODE=false  # Start without EE for simpler testing
export MAKER_NUM_CANDIDATES=3  # Reduce for faster testing
```

### 4. Start Orchestrator
```bash
# In a separate terminal
cd /Users/anthonylui/BreakingWind
source venv/bin/activate  # Or your virtualenv
python orchestrator/api_server.py

# Verify orchestrator
curl -s http://localhost:8080/health
# Expected: {"status": "healthy", "enable_skills": true, "enable_long_running": true}
```

---

## Test Suite 1: Phase 1 - Long-Running Support

### Test 1.1: ProgressTracker with Real Workflow
```bash
# Create a test task that will take multiple steps
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_progress_1",
    "user_input": "Create a simple Python function that adds two numbers"
  }'

# Expected behaviors:
# 1. workspace/claude-progress.txt should be created
# 2. Progress entries should appear in real-time
# 3. Feature should be tracked in workspace/feature_list.json

# Verify progress file
cat workspace/claude-progress.txt
# Expected: Timestamped entries showing task progress

# Verify feature list
cat workspace/feature_list.json
# Expected: JSON with feature entry for the task
```

### Test 1.2: Session Resumability
```bash
# Start a complex task
SESSION_ID="test_session_$(date +%s)"

curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d "{
    \"task_id\": \"$SESSION_ID\",
    \"user_input\": \"Write a calculator class with add, subtract, multiply, divide methods\"
  }"

# Wait for partial completion, then interrupt (Ctrl+C the orchestrator)

# Restart orchestrator
python orchestrator/api_server.py &

# Resume the session
curl -X POST http://localhost:8080/api/session/$SESSION_ID/resume-long

# Expected:
# 1. Resume context includes recent progress
# 2. Resume context includes git log
# 3. Workflow continues from where it left off
# 4. No duplicate work
```

### Test 1.3: Checkpoint Creation
```bash
# Create a simple task that should pass tests
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_checkpoint_1",
    "user_input": "Write a function that returns True"
  }'

# After completion, create checkpoint
curl -X POST http://localhost:8080/api/session/test_checkpoint_1/checkpoint \
  -H "Content-Type: application/json" \
  -d '{"feature_name": "simple_function"}'

# Expected:
# 1. Git commit is created
# 2. Commit message includes "feat: Complete simple_function"
# 3. Commit message includes "Generated by MAKER"
# 4. Feature status updated to passes=true in feature_list.json

# Verify git commit
git log -1 --oneline | grep "simple_function"
# Expected: Commit with feature name
```

---

## Test Suite 2: Phase 2 - Skills Framework

### Test 2.1: Skill Loading
```bash
# Verify skills are loaded on startup
curl -s http://localhost:8080/api/skills/list

# Expected: List of 5 skills:
# - regex-pattern-fixing
# - test-driven-bug-fixing
# - python-ast-refactoring
# - error-message-reading
# - django-migration-patterns
```

### Test 2.2: Skill Matching (Regex Task)
```bash
# Create a task that should trigger regex skill
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_skill_regex",
    "user_input": "Fix this email validation regex: r\"[\\w.]+@[\\w.]+\" to properly validate emails"
  }'

# Expected behaviors:
# 1. SkillMatcher detects "regex" keyword
# 2. regex-pattern-fixing skill is loaded
# 3. Skill instructions are injected into Coder agent prompt
# 4. Generated code uses proper email regex pattern
# 5. Skill usage is tracked in Redis

# Verify skill was used
docker compose exec redis redis-cli GET skills:usage:regex-pattern-fixing
# Expected: Number > 0
```

### Test 2.3: Skill Matching (Test-Driven Task)
```bash
# Create a task that should trigger test-driven skill
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_skill_tdd",
    "user_input": "Fix the failing test in test_calculator.py - the add() method is returning the wrong result"
  }'

# Expected:
# 1. test-driven-bug-fixing skill is matched
# 2. Agent follows test-first approach
# 3. Fix is minimal and targeted
# 4. PASS_TO_PASS tests still pass

# Verify skill usage
docker compose exec redis redis-cli GET skills:usage:test-driven-bug-fixing
# Expected: Number > 0
```

### Test 2.4: Multiple Skills (AST + Error Reading)
```bash
# Create a task that might match multiple skills
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_skill_multi",
    "user_input": "Fix the AST parsing error: AttributeError: \"NoneType\" object has no attribute \"body\""
  }'

# Expected:
# 1. Both python-ast-refactoring AND error-message-reading skills matched
# 2. Top 2 skills (by relevance) are injected
# 3. Agent uses both skill contexts
# 4. Error is fixed using error message guidance
# 5. AST manipulation follows best practices

# Verify both skills were used
docker compose exec redis redis-cli MGET \
  skills:usage:python-ast-refactoring \
  skills:usage:error-message-reading
# Expected: Both > 0
```

---

## Test Suite 3: End-to-End Integration

### Test 3.1: Complete Workflow with Skills and Checkpointing
```bash
SESSION_ID="e2e_test_$(date +%s)"

# 1. Start task with skill matching
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d "{
    \"task_id\": \"$SESSION_ID\",
    \"user_input\": \"Create a URL validator using regex. It should validate http and https URLs.\"
  }"

# Wait for completion

# 2. Create checkpoint
curl -X POST http://localhost:8080/api/session/$SESSION_ID/checkpoint \
  -H "Content-Type: application/json" \
  -d '{"feature_name": "url_validator"}'

# 3. Verify complete workflow
echo "=== Progress Log ==="
cat workspace/claude-progress.txt | tail -20

echo "=== Feature List ==="
cat workspace/feature_list.json | jq .

echo "=== Git Commit ==="
git log -1 --format="%s%n%b"

echo "=== Skill Usage ==="
docker compose exec redis redis-cli GET skills:usage:regex-pattern-fixing

# Expected full workflow:
# 1. Progress tracked throughout
# 2. Regex skill matched and used
# 3. Code generated with proper URL regex
# 4. Tests pass
# 5. Git commit created
# 6. Feature marked as complete
# 7. Skill usage incremented
```

### Test 3.2: Resume After Interruption with Skills
```bash
SESSION_ID="resume_test_$(date +%s)"

# 1. Start complex task
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d "{
    \"task_id\": \"$SESSION_ID\",
    \"user_input\": \"Create a Django model for User with email validation using regex, write migrations, and add unit tests\"
  }" &

# 2. Wait 30 seconds, then kill orchestrator
sleep 30
killall python  # Or Ctrl+C the orchestrator terminal

# 3. Restart orchestrator
python orchestrator/api_server.py &
sleep 5

# 4. Resume session
curl -X POST http://localhost:8080/api/session/$SESSION_ID/resume-long

# Expected:
# 1. Multiple skills matched (regex, django-migration, test-driven)
# 2. Resume context includes partial progress
# 3. Work continues without duplication
# 4. All skills are re-applied
# 5. Final checkpoint succeeds
```

---

## Validation Criteria

### Phase 1 Pass Criteria
- [ ] Progress logged to `workspace/claude-progress.txt` with timestamps
- [ ] Features tracked in `workspace/feature_list.json` with correct structure
- [ ] Sessions resume with correct context (progress + git log)
- [ ] Checkpoints create git commits with proper format
- [ ] Tests verified before checkpointing (real pytest/unittest run)
- [ ] No crashes or errors in orchestrator logs

### Phase 2 Pass Criteria
- [ ] All 5 skills load successfully on startup
- [ ] Skills are matched correctly based on keywords
- [ ] Skill instructions appear in agent prompts (check logs)
- [ ] Generated code shows evidence of using skill guidance
- [ ] Skill usage tracked in Redis
- [ ] Multiple skills can be applied to single task
- [ ] Skill context improves code quality (vs no-skill baseline)

### Performance Metrics
- [ ] Task completion time < 5 minutes for simple tasks
- [ ] Skill matching adds < 1 second overhead
- [ ] Progress tracking adds < 100ms overhead
- [ ] Checkpoint creation completes in < 10 seconds
- [ ] Resume reloads context in < 2 seconds

---

## Failure Scenarios to Test

### 1. Missing Dependencies
```bash
# Test without PyYAML
pip uninstall -y pyyaml
python orchestrator/api_server.py
# Expected: Clear error message about missing PyYAML
pip install pyyaml==6.0.1
```

### 2. Skills Directory Missing
```bash
# Remove skills directory
mv skills skills_backup
python orchestrator/api_server.py
# Expected: Warning logged, skills framework disabled gracefully
mv skills_backup skills
```

### 3. Redis Down
```bash
# Stop Redis
docker compose stop redis
curl -X POST http://localhost:8080/api/orchestrate -d '{"task_id": "test", "user_input": "test"}'
# Expected: Graceful degradation, task runs without skill tracking
docker compose start redis
```

### 4. Tests Fail During Checkpoint
```bash
# Create intentionally broken code
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_fail",
    "user_input": "Write a function that always raises an exception"
  }'

# Try to checkpoint
curl -X POST http://localhost:8080/api/session/test_fail/checkpoint \
  -d '{"feature_name": "broken_code"}'

# Expected: Checkpoint FAILS with error about tests not passing
# Feature should NOT be marked as complete
```

---

## Manual Verification Steps

### 1. Check Skill Context in Logs
```bash
# Enable debug logging
export LOG_LEVEL=DEBUG

# Run a regex task
curl -X POST http://localhost:8080/api/orchestrate \
  -H "Content-Type: application/json" \
  -d '{"task_id": "debug_test", "user_input": "Fix email regex"}'

# Check orchestrator logs for:
tail -f orchestrator.log | grep -A 50 "Available Proven Patterns"
# Expected: Skill instructions visible in prompt sent to Coder agent
```

### 2. Compare Output Quality (With vs Without Skills)
```bash
# Baseline (no skills)
export ENABLE_SKILLS=false
curl -X POST http://localhost:8080/api/orchestrate \
  -d '{"task_id": "baseline", "user_input": "Create email validator"}' \
  > /tmp/baseline_output.txt

# With skills
export ENABLE_SKILLS=true
curl -X POST http://localhost:8080/api/orchestrate \
  -d '{"task_id": "skilled", "user_input": "Create email validator"}' \
  > /tmp/skilled_output.txt

# Compare outputs
diff /tmp/baseline_output.txt /tmp/skilled_output.txt
# Expected: Skilled version uses better regex patterns from skill guidance
```

### 3. Inspect Redis Data
```bash
# Check all skill usage
docker compose exec redis redis-cli KEYS "skills:*"

# Get usage stats
docker compose exec redis redis-cli --scan --pattern "skills:usage:*" | \
  xargs -I {} docker compose exec redis redis-cli GET {}

# Expected: Usage counts for each used skill
```

---

## Test Execution Checklist

**Before running tests:**
- [ ] All 6 llama.cpp models running (`ps aux | grep llama-server`)
- [ ] Redis running (`docker compose ps redis`)
- [ ] MCP server running (`docker compose ps mcp-server`)
- [ ] Orchestrator running (`curl localhost:8080/health`)
- [ ] Workspace and skills directories exist
- [ ] Environment variables set correctly

**Run tests in order:**
1. [ ] Test Suite 1: Phase 1 (Long-Running Support)
2. [ ] Test Suite 2: Phase 2 (Skills Framework)
3. [ ] Test Suite 3: End-to-End Integration
4. [ ] Failure Scenarios
5. [ ] Manual Verification

**After tests:**
- [ ] Review orchestrator logs for errors
- [ ] Check workspace/claude-progress.txt for complete workflow
- [ ] Verify git commits are clean
- [ ] Confirm Redis has skill usage data
- [ ] Document any failures or unexpected behavior

---

## Expected Timeline

- **Setup (models + services):** 10-15 minutes
- **Test Suite 1 (Phase 1):** 15-20 minutes
- **Test Suite 2 (Phase 2):** 20-25 minutes
- **Test Suite 3 (E2E):** 30-40 minutes
- **Failure scenarios:** 10-15 minutes
- **Manual verification:** 15-20 minutes

**Total: ~2 hours for complete validation**

---

## Success Criteria

âœ… **Phase 1 & 2 are production-ready when:**
1. All integration tests pass with real models
2. No errors in orchestrator logs during normal operation
3. Skills measurably improve code quality (manual inspection)
4. Progress tracking works across session restarts
5. Checkpoints create valid git commits
6. Skill usage is tracked correctly in Redis
7. System gracefully handles failures (Redis down, missing skills, etc.)

If any test fails, it's NOT production-ready and needs fixes before Phase 3.
