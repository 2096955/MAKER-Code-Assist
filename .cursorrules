# Cursor AI Rules for MAKER Multi-Agent System

## Project Context

You are working on MAKER, a local multi-agent AI coding system based on Cognizant's MAKER paper (arXiv:2511.09030). The system runs on Apple Silicon (M4 Max 128GB) using llama.cpp with Metal acceleration.

**Architecture:**
- 5 AI agents: Preprocessor (Gemma2-2B), Planner (Nemotron Nano 8B), Coder (Devstral 24B), Reviewer (Qwen3-Coder 32B), Voter (Qwen2.5-1.5B)
- MAKER voting: Parallel candidate generation with first-to-K voting
- EE Memory: 4-level hierarchical memory (L₀→L₁→L₂→L₃) with narrative awareness
- Tech stack: Python 3.11+, FastAPI, Redis, Docker, llama.cpp
- Code style: Clean, minimal, type-hinted Python following existing patterns

## Current Implementation Gaps (Critical)

The system needs three major enhancements based on Anthropic best practices:

1. **Long-Running Agent Support** - Enable multi-session work with progress tracking and resumability
2. **Skills Framework** - Dynamic skill loading for domain specialization
3. **Incremental Learning** - Auto-extract and apply learned patterns from completed tasks

**Reference:** See `docs/context-engineering-skills-analysis.md` for detailed gap analysis.

## Code Style Guidelines

### Python
- Use type hints everywhere: `def func(x: str) -> Dict[str, Any]:`
- Prefer dataclasses for structured data
- Use async/await for I/O operations
- Keep functions under 50 lines
- Follow existing orchestrator patterns (see `orchestrator/orchestrator.py`)

### File Organization
- New features in separate files under appropriate directory
- Agent prompts in `agents/*.md`
- Skills in `skills/<skill-name>/SKILL.md`
- Tests in `tests/test_*.py`
- Docs in `docs/*.md`

### Naming Conventions
- Classes: `PascalCase` (e.g., `ProgressTracker`, `SkillLoader`)
- Functions/methods: `snake_case` (e.g., `load_session`, `extract_skill`)
- Constants: `UPPER_SNAKE_CASE` (e.g., `MAX_CONTEXT_TOKENS`)
- Files: `snake_case.py` (e.g., `progress_tracker.py`, `skill_loader.py`)

## Implementation Priorities

### Phase 1: Long-Running Agent Support (IMPLEMENT FIRST)
**Files to create:**
- `orchestrator/progress_tracker.py` - Progress logging and feature tracking
- `orchestrator/checkpoint_manager.py` - Clean checkpointing with git commits
- `orchestrator/session_manager.py` - Enhanced session resumability

**Files to modify:**
- `orchestrator/orchestrator.py` - Add resume_session() and checkpoint_session()
- `docker-compose.yml` - Add volume mount for progress files

**Critical patterns:**
- Use `claude-progress.txt` for append-only logs
- Use `feature_list.json` for structured feature tracking
- Git commit after each successful checkpoint
- Resumability protocol: pwd → git log → progress → next feature

### Phase 2: Skills Framework (IMPLEMENT SECOND)
**Files to create:**
- `orchestrator/skill_loader.py` - Load and discover skills
- `orchestrator/skill_matcher.py` - Find relevant skills for tasks
- `skills/README.md` - Skills documentation
- `skills/example-skill/SKILL.md` - Template skill

**Files to modify:**
- `orchestrator/orchestrator.py` - Inject skills into agent prompts
- `orchestrator/rag_service_faiss.py` - Add skills collection

**Critical patterns:**
- YAML frontmatter: `---\nname: skill-name\ndescription: ...\n---`
- Skills indexed in RAG for semantic search
- Dynamic skill loading based on task context
- Skills augment system prompts, don't replace them

### Phase 3: Incremental Learning (IMPLEMENT THIRD)
**Files to create:**
- `orchestrator/skill_extractor.py` - Extract skills from completed tasks
- `orchestrator/skill_registry.py` - Manage skill lifecycle
- `scripts/extract_skills_batch.py` - Batch process historical tasks

**Files to modify:**
- `orchestrator/orchestrator.py` - Call skill extraction after task completion
- `orchestrator/api_server.py` - Add skill management endpoints

**Critical patterns:**
- Extract skills only from approved, non-trivial tasks
- Auto-generate SKILL.md from task artifacts
- Semantic similarity threshold: 0.85 for auto-apply
- Skill evolution: merge learnings from multiple instances

## Integration Points

### With Existing EE Memory
**IMPORTANT:** Skills and EE Memory are SEPARATE but complementary:

- **EE Memory (Melodic Lines)**: Business narratives across codebase (e.g., "Payment Processing Flow")
- **Skills**: Coding patterns from SWE-bench experience (e.g., "How to fix regex bugs")

**In practice:**
```python
# Both are used together in agent prompts:
narrative_context = agent_memory.get_context_for_agent(task)  # EE Memory
skill_context = skill_matcher.find_relevant_skills(task)      # Skills

# Combined prompt includes both:
system_prompt = base + "\n# Narratives\n" + narrative_context + "\n# Patterns\n" + skill_context
```

**SkillMatcher uses RAG directly**, NOT EE World Model (different collections)

### With MAKER Voting
- Skill-augmented candidates should score higher in voting
- Track skill usage → outcome correlation
- Use in confidence calibration

### With SWE-bench Evaluation
- Skills should improve resolve rate on repeated patterns
- Track skill impact in metrics
- Expect >10% improvement on similar tasks

## Common Pitfalls to Avoid

1. **Don't break existing functionality** - All changes must be backward compatible
2. **Don't hardcode paths** - Use `os.getenv()` with defaults
3. **Don't skip error handling** - Every external call needs try/except
4. **Don't forget tests** - Add tests for new classes in `tests/`
5. **Don't ignore types** - Use mypy-compatible type hints
6. **Don't duplicate state** - Redis is source of truth, files are snapshots

## Testing Requirements

### For Each New Feature
- Unit tests in `tests/test_<feature>.py`
- Integration test showing end-to-end flow
- Manual test with actual orchestrator

### Test Patterns
```python
import pytest
from orchestrator.progress_tracker import ProgressTracker

def test_log_progress(tmp_path):
    tracker = ProgressTracker(tmp_path)
    tracker.log_progress("Test message")
    assert "Test message" in tracker.progress_file.read_text()
```

## Documentation Requirements

### For Each New Module
- Docstring at module level explaining purpose
- Docstrings for all public classes/methods
- Type hints for all function signatures
- Example usage in docstring

### Example
```python
"""
Progress tracking for long-running agent sessions.

Implements Anthropic's pattern of structured progress files:
- claude-progress.txt: Append-only log of accomplishments
- feature_list.json: Structured feature checklist with pass/fail status

Usage:
    tracker = ProgressTracker(workspace_path)
    tracker.log_progress("Completed user authentication")
    tracker.update_feature_status("auth", passes=True)
"""
```

## Redis Schema

### Existing Keys
- `task:{task_id}` - TaskState for active task
- `session:{session_id}` - ContextCompressor state

### New Keys (Add These)
- `progress:{session_id}` - ProgressTracker state
- `skills:registry` - Skill registry metadata
- `skills:usage:{skill_name}` - Usage statistics

## API Endpoints to Add

```python
# In orchestrator/api_server.py

@app.post("/api/session/{session_id}/resume")
async def resume_session(session_id: str):
    """Resume long-running session"""

@app.post("/api/session/{session_id}/checkpoint")
async def checkpoint_session(session_id: str, feature_name: str):
    """Create clean checkpoint"""

@app.get("/api/skills")
async def list_skills():
    """List available skills"""

@app.post("/api/skills/extract/{task_id}")
async def extract_skill(task_id: str):
    """Extract skill from completed task"""
```

## Git Commit Standards

When implementing:
- Commit after each phase (not each file)
- Use conventional commits: `feat:`, `fix:`, `docs:`, `test:`
- Reference issue/plan in commit message

Example:
```
feat(long-running): Add progress tracking and resumability

- Implement ProgressTracker class with feature_list.json
- Add resume_session() to orchestrator
- Add checkpoint_manager with git integration

Addresses Phase 1 of context-engineering-skills plan
```

## Environment Variables

### Existing
- `EE_MODE=true` - Enable EE Memory planner
- `MAKER_NUM_CANDIDATES=5` - Voting candidates
- `MAX_CONTEXT_TOKENS=32000` - Context limit

### New (Add These)
- `ENABLE_LONG_RUNNING=true` - Enable progress tracking
- `ENABLE_SKILLS=true` - Enable skills framework
- `ENABLE_SKILL_LEARNING=true` - Auto-extract skills
- `SKILLS_DIR=/app/skills` - Skills directory path (inside container)
- `WORKSPACE_DIR=/app/workspace` - Workspace for progress files (inside container)

### Directory Setup
```bash
# Create on host (before first run):
mkdir -p workspace skills

# Docker volume mounts (in docker-compose.yml):
volumes:
  - ./workspace:/app/workspace
  - ./skills:/app/skills

# Git ignore workspace, keep skills:
# .gitignore:
workspace/*
!workspace/.gitkeep
```

## When in Doubt

1. Check existing patterns in `orchestrator/orchestrator.py`
2. Follow Anthropic patterns from `docs/context-engineering-skills-analysis.md`
3. Maintain backward compatibility (feature flags for new behavior)
4. Write tests before implementation (TDD)
5. Ask for clarification rather than guessing

## Success Criteria

### Phase 1 Complete When:
- [ ] Can resume interrupted tasks across sessions
- [ ] Progress logged to claude-progress.txt
- [ ] Feature checklist tracked in feature_list.json
- [ ] Git commits created at checkpoints
- [ ] Tests pass for session resumability

### Phase 2 Complete When:
- [ ] Skills load dynamically from SKILL.md files
- [ ] RAG finds relevant skills for tasks
- [ ] Skills augment agent system prompts
- [ ] Example skill (XML→SPSS) works end-to-end
- [ ] Tests pass for skill loading and matching

### Phase 3 Complete When:
- [ ] Skills auto-extracted from completed tasks
- [ ] Skill registry tracks usage and success rate
- [ ] Auto-apply detects "I've done this before"
- [ ] Second XML→SPSS task uses learned skill
- [ ] SWE-bench resolve rate improves >10% on similar tasks

## Performance Targets

- Session resume: <2s to load state
- Skill matching: <500ms for semantic search
- Skill extraction: <10s per task
- No regression in existing task execution time

## Code Review Checklist

Before marking complete:
- [ ] Type hints on all functions
- [ ] Docstrings on all public APIs
- [ ] Error handling for external calls
- [ ] Tests written and passing
- [ ] No hardcoded paths or credentials
- [ ] Backward compatible (feature flags)
- [ ] Documentation updated
- [ ] Git commit with conventional format

---

**Remember:** This is extending an existing system, not building from scratch. Preserve all existing functionality while adding new capabilities.
