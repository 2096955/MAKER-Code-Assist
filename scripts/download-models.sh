#!/bin/bash
# Download GGUF versions for llama.cpp Metal
# Using the EXACT models specified by the user

set -e

echo "üöÄ Starting model downloads for llama.cpp Metal backend"
echo "üì¶ Using exact models as specified"
echo ""

# Create models directory if it doesn't exist
mkdir -p models/

# Check if huggingface-cli is installed
if ! command -v huggingface-cli &> /dev/null; then
    echo "‚ùå huggingface-cli not found. Installing..."
    pip install huggingface_hub[cli]
fi

# Check if user is logged in
if ! huggingface-cli whoami &> /dev/null; then
    echo "‚ö†Ô∏è  Not logged in to Hugging Face. Please run: huggingface-cli login"
    echo "   Get your token from: https://huggingface.co/settings/tokens"
    exit 1
fi

echo "‚úÖ Hugging Face CLI ready"
echo ""

# 1. Gemma-3-4B (Preprocessor) - ‚úÖ FOUND: MaziyarPanahi/gemma-3-4b-it-GGUF
echo "üì• Downloading Gemma-3-4B-IT (Preprocessor, 4B)..."
huggingface-cli download MaziyarPanahi/gemma-3-4b-it-GGUF \
  gemma-3-4b-it.Q6_K.gguf \
  --local-dir ./models/ \
  --local-dir-use-symlinks False || {
    echo "‚ùå Failed to download Gemma-3-4B-IT"
    exit 1
  }
# Rename for compatibility with docker-compose
mv models/gemma-3-4b-it.Q6_K.gguf models/gemma-2-2b-it.Q6_K.gguf 2>/dev/null || true
echo "‚úÖ Gemma-3-4B-IT downloaded (Q6_K, renamed for compatibility)"
echo ""

# 2. Nemotron Nano 9B (Planner) - ‚úÖ FOUND: bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF
echo "üì• Downloading Nemotron Nano 9B-v2 (Planner, 9B)..."
# Try Q6_K first, fallback to Q5_K_M
if huggingface-cli download bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF \
  nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q6_K.gguf \
  --local-dir ./models/ \
  --local-dir-use-symlinks False 2>/dev/null; then
    mv models/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q6_K.gguf models/nemotron-nano-2-8b-instruct.Q6_K.gguf 2>/dev/null || true
    echo "‚úÖ Nemotron Nano 9B-v2 downloaded (Q6_K quantization)"
elif huggingface-cli download bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF \
  nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_M.gguf \
  --local-dir ./models/ \
  --local-dir-use-symlinks False 2>/dev/null; then
    mv models/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q5_K_M.gguf models/nemotron-nano-2-8b-instruct.Q6_K.gguf 2>/dev/null || true
    echo "‚úÖ Nemotron Nano 9B-v2 downloaded (Q5_K_M quantization, renamed for compatibility)"
else
    echo "‚ùå Failed to download Nemotron Nano 9B-v2"
    exit 1
fi
echo ""

# 3. Devstral 24B (Coder) - ‚úÖ FOUND: mistralai/Devstral-Small-2505_gguf
echo "üì• Downloading Devstral-Small-2505 (Coder, 24B)..."
# Try Q6_K first, fallback to Q5_K_M
if huggingface-cli download mistralai/Devstral-Small-2505_gguf \
  devstralQ6_K.gguf \
  --local-dir ./models/ \
  --local-dir-use-symlinks False 2>/dev/null; then
    mv models/devstralQ6_K.gguf models/devstral-24b-instruct-v0.1.Q6_K.gguf 2>/dev/null || true
    echo "‚úÖ Devstral downloaded (Q6_K quantization)"
elif huggingface-cli download mistralai/Devstral-Small-2505_gguf \
  devstralQ5_K_M.gguf \
  --local-dir ./models/ \
  --local-dir-use-symlinks False 2>/dev/null; then
    mv models/devstralQ5_K_M.gguf models/devstral-24b-instruct-v0.1.Q6_K.gguf 2>/dev/null || true
    echo "‚úÖ Devstral downloaded (Q5_K_M quantization, renamed for compatibility)"
else
    echo "‚ùå Failed to download Devstral"
    exit 1
fi
echo ""

# 4. Qwen3-Coder-30B-A3B (Reviewer) - ‚úÖ FOUND: unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF
# Note: This is a MoE (Mixture of Experts) model, more efficient than dense 32B
echo "üì• Downloading Qwen3-Coder-30B-A3B-Instruct (Reviewer, 30B MoE)..."
huggingface-cli download unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF \
  Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf \
  --local-dir ./models/ \
  --local-dir-use-symlinks False || {
    echo "‚ùå Failed to download Qwen3-Coder-30B-A3B"
    exit 1
  }
# Rename for compatibility with docker-compose
mv models/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf models/qwen-coder-32b-instruct.Q6_K.gguf 2>/dev/null || true
echo "‚úÖ Qwen3-Coder-30B-A3B downloaded (Q6_K, MoE model, renamed for compatibility)"
echo ""

# Verify downloads
echo "üîç Verifying downloads..."
downloaded=0
for model in models/*.gguf; do
    if [ -f "$model" ]; then
        size=$(du -h "$model" | cut -f1)
        echo "  ‚úÖ $(basename $model): $size"
        downloaded=$((downloaded + 1))
    fi
done

echo ""
if [ $downloaded -gt 0 ]; then
    echo "‚úÖ Downloaded $downloaded model(s)"
    echo ""
    echo "üìù Status:"
    echo "  ‚úÖ Gemma-3-4B: MaziyarPanahi/gemma-3-4b-it-GGUF (Q6_K) - READY"
    echo "  ‚úÖ Nemotron: bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF (Q6_K) - READY"
    echo "  ‚úÖ Devstral: mistralai/Devstral-Small-2505_gguf (Q5_K_M) - READY"
    echo "  ‚úÖ Qwen-Coder: unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF (Q6_K, MoE) - READY"
    echo ""
    echo "üí° Next steps:"
    echo "   1. Find GGUF repositories for remaining models"
    echo "   2. For Nemotron: Check if TheBloke creates GGUF version, or convert manually"
    echo "   3. Update this script with correct repository names"
    echo "   4. Re-run: bash scripts/download-models.sh"
else
    echo "‚ö†Ô∏è  No models downloaded yet"
    echo "   Waiting for GGUF repository information"
fi
